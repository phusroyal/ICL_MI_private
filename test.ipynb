{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phu.hoang/.conda/envs/icl_mi/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Model, GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-xl')\n",
    "model = GPT2Model.from_pretrained('gpt2-xl')\n",
    "\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Model(\n",
       "  (wte): Embedding(50257, 1600)\n",
       "  (wpe): Embedding(1024, 1600)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (h): ModuleList(\n",
       "    (0-47): 48 x GPT2Block(\n",
       "      (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2SdpaAttention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (act): NewGELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/s6-HH/ent/selection_15.npy'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'data/s6-HH/selection_15.json'\n",
    "path.split('/se')[0]+'/ent/'+path.split('/')[-1][:-5]+'.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'datas6-HHentselection_15.json'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path.split('/')[0] + path.split('/')[1] + 'ent' + path.split('/')[2]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phu.hoang/.conda/envs/icl_mi/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Model, GPT2Tokenizer\n",
    "\n",
    "class GPT2WithBlockIO(GPT2Model):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.block_inputs = []\n",
    "        self.block_outputs = []\n",
    "\n",
    "        # Register hooks for each GPT2Block\n",
    "        for i, block in enumerate(self.h):\n",
    "            block.register_forward_hook(self._get_block_io_hook(i))\n",
    "\n",
    "    def _get_block_io_hook(self, block_idx):\n",
    "        def hook(module, input, output):\n",
    "            # Save the input and output of the block\n",
    "            self.block_inputs.append(input)\n",
    "            self.block_outputs.append(output)\n",
    "        return hook\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        # Clear previous inputs and outputs\n",
    "        self.block_inputs = []\n",
    "        self.block_outputs = []\n",
    "        \n",
    "        # Call the superclass forward method\n",
    "        return super().forward(*args, **kwargs)\n",
    "\n",
    "import torch\n",
    "from transformers import GPT2Model, GPT2Tokenizer\n",
    "\n",
    "# Load the tokenizer and model for GPT-2 XL\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-xl')\n",
    "model = GPT2WithBlockIO.from_pretrained('gpt2-xl')\n",
    "\n",
    "# # Move model to device if using GPU\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model.to(device)\n",
    "\n",
    "# Prepare input data\n",
    "input_text = \"Hello, my dog is cute\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "\n",
    "# Perform a forward pass\n",
    "outputs = model(input_ids)\n",
    "\n",
    "# # Access the outputs\n",
    "# for layer_idx in range(len(model.h)):\n",
    "#     input = model.block_inputs[layer_idx]  # Input to layer_idx\n",
    "#     output = model.block_outputs[layer_idx]  # Output of layer_idx\n",
    "    \n",
    "#     print(f'input shape: {input[0].shape}')\n",
    "#     print(f'output shape: {output[0].shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.block_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.3794, -0.5317,  0.0160,  ..., -0.3212, -0.3500,  0.0559],\n",
       "          [ 0.1626, -0.0287,  0.2274,  ...,  0.2101,  0.3903, -1.1122],\n",
       "          [ 0.1100, -0.9780, -1.0651,  ..., -0.9416, -0.7865, -1.9516],\n",
       "          [ 0.7601,  0.2530,  0.6056,  ...,  0.0998,  0.1140,  0.1713],\n",
       "          [-0.4443, -0.7192, -0.4717,  ..., -0.4864, -0.4004, -0.0764],\n",
       "          [ 0.1119,  0.2270,  0.6549,  ...,  0.4585,  0.0979,  0.3265]],\n",
       "\n",
       "         [[-0.2735,  0.0655,  0.2582,  ..., -0.6297, -0.2308, -0.4513],\n",
       "          [ 0.1930, -0.3743, -0.3185,  ..., -0.4061,  0.1120, -0.4566],\n",
       "          [ 0.3378, -0.5363, -0.6307,  ...,  0.3724, -0.0900,  0.0847],\n",
       "          [-0.5086,  0.2162,  0.1905,  ..., -0.0044, -0.0570,  0.4325],\n",
       "          [ 0.0397, -0.1924, -1.7286,  ...,  0.9129, -0.2322,  0.2835],\n",
       "          [-0.6634,  0.2892,  0.7420,  ...,  0.1586, -0.3098, -0.3302]],\n",
       "\n",
       "         [[-0.2421,  0.2638,  0.9632,  ..., -0.7105, -0.5454, -0.6864],\n",
       "          [-0.4820, -0.3488,  0.0207,  ..., -0.5639,  0.3663, -0.1607],\n",
       "          [ 0.2691,  0.3860,  0.0821,  ..., -0.2458,  0.3344, -0.0027],\n",
       "          [ 0.4578, -0.1893, -0.1036,  ..., -0.2313, -0.1228, -0.1495],\n",
       "          [-0.2341,  0.0758,  0.4655,  ...,  0.2247,  0.9070, -0.0299],\n",
       "          [ 0.2944, -0.1686,  0.2504,  ..., -1.0104,  0.0137, -0.4316]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.1040,  0.1596,  0.0542,  ...,  0.3314, -0.0368, -0.4115],\n",
       "          [-0.0830, -0.0184,  0.2268,  ..., -0.2193,  0.3947,  0.6596],\n",
       "          [ 0.2655, -1.0208,  0.0698,  ...,  0.7083,  0.1159,  0.2666],\n",
       "          [ 0.2442, -0.6205,  0.2420,  ...,  0.0216, -0.1219,  0.0497],\n",
       "          [ 0.1272,  0.2484,  0.5703,  ...,  0.0137, -0.4978, -0.6553],\n",
       "          [ 0.5305, -0.0421,  0.6005,  ..., -0.1116,  0.1888, -0.4953]],\n",
       "\n",
       "         [[ 0.0694, -0.2292,  0.1716,  ..., -0.4688, -0.1241,  0.3715],\n",
       "          [-0.3272,  0.1800, -0.1340,  ..., -0.3234,  0.4551,  0.0879],\n",
       "          [-0.0110, -0.0342,  0.5034,  ...,  0.1579,  0.0538, -0.5026],\n",
       "          [ 0.3398, -0.1077,  0.3129,  ..., -0.6993,  0.3564,  0.3910],\n",
       "          [-0.3572, -0.2008, -0.5246,  ..., -0.6763, -1.0040,  0.4576],\n",
       "          [ 0.0941,  0.2139,  0.1176,  ..., -0.5796,  0.0799,  0.1764]],\n",
       "\n",
       "         [[ 0.2470,  0.2344, -0.1849,  ..., -0.0952,  0.2827,  0.2350],\n",
       "          [ 0.3250, -0.0810,  0.1970,  ...,  0.4225, -0.1639,  0.2564],\n",
       "          [ 0.1507, -0.0670, -0.1032,  ..., -0.1281,  0.0585,  0.2256],\n",
       "          [ 0.3212,  0.1913, -0.3222,  ..., -0.3967, -0.8868,  0.0106],\n",
       "          [-0.4779, -0.3176, -0.8202,  ..., -1.0444,  0.1973, -1.1317],\n",
       "          [ 0.4213,  0.3772, -0.6371,  ..., -0.2259, -0.1218, -0.8326]]]],\n",
       "       grad_fn=<PermuteBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tup[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "for tup in output:\n",
    "    print(tup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 2.2302279e-01  1.8874916e-01  1.9327319e-01 ...  1.3554430e-01\n",
      "   -1.4152056e+00  6.1451828e-01]\n",
      "  [-3.2874677e-01 -4.0013835e-02  5.8968991e-02 ... -7.3593068e-01\n",
      "   -1.9495469e-01 -5.4683471e-01]]\n",
      "\n",
      " [[ 4.0189353e-01  3.6662313e-01 -2.5075650e-01 ...  2.3935412e-01\n",
      "   -1.7156034e+00  5.6176621e-01]\n",
      "  [-2.6284853e-01  2.0323552e-02 -1.5000072e-01 ... -1.1255181e+00\n",
      "   -8.6741738e-02 -5.5752051e-01]]\n",
      "\n",
      " [[ 1.0387648e+00  9.7636062e-01 -5.1620948e-01 ... -1.4057718e-01\n",
      "   -2.1595349e+00  7.1170121e-01]\n",
      "  [ 1.3679866e-01 -9.8974064e-02 -8.2280487e-02 ... -1.4746170e+00\n",
      "   -4.5692396e-01 -9.2212564e-01]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 1.6231673e+00  3.2004982e-01  8.4644061e-01 ... -1.6476421e+02\n",
      "    3.6370623e+00  2.6803994e-01]\n",
      "  [-2.0295053e+00  9.3292773e-02  2.0244243e+00 ... -9.8992027e+01\n",
      "    5.4953299e+00  4.0980244e+00]]\n",
      "\n",
      " [[ 2.5936060e+00  2.9077780e-01  8.8665712e-01 ... -1.7465759e+02\n",
      "    2.8549776e+00  5.8863461e-01]\n",
      "  [-2.0312424e+00  8.2391447e-01  2.7341540e+00 ... -1.3491873e+02\n",
      "    6.8711710e+00  4.4640589e+00]]\n",
      "\n",
      " [[ 4.2178097e+00 -2.3562253e-01  6.2293464e-01 ... -1.4587975e+02\n",
      "    2.1086733e+00  1.7707446e+00]\n",
      "  [-1.0869865e+00  1.0332072e+00  4.5278783e+00 ... -1.3482539e+02\n",
      "    7.7891498e+00  4.3971519e+00]]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "indices = [0,2]\n",
    "\n",
    "# Extract the last tensor from each tuple and stack into a NumPy array\n",
    "numpy_array = torch.stack([tup[0].detach().reshape(-1, 1600)[indices] \\\n",
    "                            for tup in model.block_outputs]).numpy()\n",
    "\n",
    "print(numpy_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48, 6, 1600)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [7, 8, 9]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Example 2D tensor\n",
    "tensor = torch.tensor([[1, 2, 3],\n",
    "                       [4, 5, 6],\n",
    "                       [7, 8, 9]])\n",
    "\n",
    "# List of indices to extract from the 1st dimension\n",
    "indices = [0, 2]\n",
    "\n",
    "# Extract data using advanced indexing\n",
    "extracted = tensor[indices]\n",
    "\n",
    "print(extracted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor2numpy(tensor):\n",
    "\treturn tensor.cpu().detach().numpy()\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# create blank numpy array\n",
    "nui = []\n",
    "\n",
    "nui.append(tensor2numpy(niu))\n",
    "nui.append(tensor2numpy(niu2))\n",
    "\n",
    "# convert to numpy array\n",
    "nui = np.array(nui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 6, 1600)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nui.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 6, 1600)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nui.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 1600])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.block_outputs[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phu.hoang/.conda/envs/icl_mi/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0 hidden state shape: torch.Size([1, 6, 1600])\n",
      "Layer 1 hidden state shape: torch.Size([1, 6, 1600])\n",
      "Layer 2 hidden state shape: torch.Size([1, 6, 1600])\n",
      "Layer 3 hidden state shape: torch.Size([1, 6, 1600])\n",
      "Layer 4 hidden state shape: torch.Size([1, 6, 1600])\n",
      "Layer 5 hidden state shape: torch.Size([1, 6, 1600])\n",
      "Layer 6 hidden state shape: torch.Size([1, 6, 1600])\n",
      "Layer 7 hidden state shape: torch.Size([1, 6, 1600])\n",
      "Layer 8 hidden state shape: torch.Size([1, 6, 1600])\n",
      "Layer 9 hidden state shape: torch.Size([1, 6, 1600])\n",
      "Layer 10 hidden state shape: torch.Size([1, 6, 1600])\n",
      "Layer 11 hidden state shape: torch.Size([1, 6, 1600])\n",
      "Layer 12 hidden state shape: torch.Size([1, 6, 1600])\n",
      "Layer 13 hidden state shape: torch.Size([1, 6, 1600])\n",
      "Layer 14 hidden state shape: torch.Size([1, 6, 1600])\n",
      "Layer 15 hidden state shape: torch.Size([1, 6, 1600])\n",
      "Layer 16 hidden state shape: torch.Size([1, 6, 1600])\n",
      "Layer 17 hidden state shape: torch.Size([1, 6, 1600])\n",
      "Layer 18 hidden state shape: torch.Size([1, 6, 1600])\n",
      "Layer 19 hidden state shape: torch.Size([1, 6, 1600])\n",
      "Layer 20 hidden state shape: torch.Size([1, 6, 1600])\n",
      "Layer 21 hidden state shape: torch.Size([1, 6, 1600])\n",
      "Layer 22 hidden state shape: torch.Size([1, 6, 1600])\n",
      "Layer 23 hidden state shape: torch.Size([1, 6, 1600])\n",
      "Layer 24 hidden state shape: torch.Size([1, 6, 1600])\n",
      "Layer 25 hidden state shape: torch.Size([1, 6, 1600])\n",
      "Layer 26 hidden state shape: torch.Size([1, 6, 1600])\n",
      "Layer 27 hidden state shape: torch.Size([1, 6, 1600])\n",
      "Layer 28 hidden state shape: torch.Size([1, 6, 1600])\n",
      "Layer 29 hidden state shape: torch.Size([1, 6, 1600])\n",
      "Layer 30 hidden state shape: torch.Size([1, 6, 1600])\n",
      "Layer 31 hidden state shape: torch.Size([1, 6, 1600])\n",
      "Layer 32 hidden state shape: torch.Size([1, 6, 1600])\n",
      "Layer 33 hidden state shape: torch.Size([1, 6, 1600])\n",
      "Layer 34 hidden state shape: torch.Size([1, 6, 1600])\n",
      "Layer 35 hidden state shape: torch.Size([1, 6, 1600])\n",
      "Layer 36 hidden state shape: torch.Size([1, 6, 1600])\n",
      "Layer 37 hidden state shape: torch.Size([1, 6, 1600])\n",
      "Layer 38 hidden state shape: torch.Size([1, 6, 1600])\n",
      "Layer 39 hidden state shape: torch.Size([1, 6, 1600])\n",
      "Layer 40 hidden state shape: torch.Size([1, 6, 1600])\n",
      "Layer 41 hidden state shape: torch.Size([1, 6, 1600])\n",
      "Layer 42 hidden state shape: torch.Size([1, 6, 1600])\n",
      "Layer 43 hidden state shape: torch.Size([1, 6, 1600])\n",
      "Layer 44 hidden state shape: torch.Size([1, 6, 1600])\n",
      "Layer 45 hidden state shape: torch.Size([1, 6, 1600])\n",
      "Layer 46 hidden state shape: torch.Size([1, 6, 1600])\n",
      "Layer 47 hidden state shape: torch.Size([1, 6, 1600])\n",
      "Layer 48 hidden state shape: torch.Size([1, 6, 1600])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Model, GPT2Tokenizer\n",
    "\n",
    "# Load the tokenizer and GPT-2 model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-xl\")\n",
    "model1 = GPT2Model.from_pretrained(\"gpt2-xl\")\n",
    "\n",
    "# Prepare input data\n",
    "input_text = \"Hello, my dog is cute\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\")[\"input_ids\"]\n",
    "\n",
    "# Perform a forward pass with `output_hidden_states=True`\n",
    "outputs = model1(input_ids, output_hidden_states=True)\n",
    "\n",
    "# Extract hidden states\n",
    "hidden_states = outputs.hidden_states  # Tuple of hidden states for each layer\n",
    "\n",
    "# Each hidden state corresponds to the output of a GPT block\n",
    "for layer_idx, layer_hidden_state in enumerate(hidden_states):\n",
    "    print(f\"Layer {layer_idx} hidden state shape: {layer_hidden_state.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "a1 = np.random.rand(2, 3, 4)\n",
    "a2 = np.random.rand(2, 5, 4)\n",
    "\n",
    "a = [a1, a2]\n",
    "\n",
    "# Save using pickle\n",
    "with open('a.pkl', 'wb') as f:\n",
    "    pickle.dump(a, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load using pickle\n",
    "with open('data/s6-HH/ent/selection_15.pkl', 'rb') as f:\n",
    "    a_loaded = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48, 112, 1600)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_loaded[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "load = np.load('a.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2, 3, 4)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2230,  0.1887,  0.1933,  ...,  0.1355, -1.4152,  0.6145],\n",
       "         [-0.2649,  0.6865, -0.1405,  ..., -0.3489, -0.5980, -0.5983],\n",
       "         [-0.3287, -0.0400,  0.0590,  ..., -0.7359, -0.1950, -0.5468],\n",
       "         [-0.6452,  0.0665,  0.6924,  ..., -0.1441, -0.0997,  0.3393],\n",
       "         [-0.2259,  0.3158, -0.0138,  ..., -0.3089, -0.1570,  0.0386],\n",
       "         [ 0.0532,  0.4883,  0.2510,  ...,  1.1421, -0.0840, -0.6012]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.block_outputs[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 1600])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.block_outputs[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 25, 6, 64])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.block_outputs[0][1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/phu.hoang/Documents/projects/ICL/ICL_MI_private/test.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://10.127.30.138:8080/home/phu.hoang/Documents/projects/ICL/ICL_MI_private/test.ipynb#Y100sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39;49mblock_outputs[\u001b[39m0\u001b[39;49m][\u001b[39m2\u001b[39;49m][\u001b[39m0\u001b[39;49m][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshape\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "model.block_outputs[0][2][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2WithBlockIO(\n",
       "  (wte): Embedding(50257, 768)\n",
       "  (wpe): Embedding(1024, 768)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (h): ModuleList(\n",
       "    (0-11): 12 x GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2SdpaAttention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (act): NewGELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check file perofmance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phu.hoang/.conda/envs/icl_mi/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2-xl')\n",
    "model = AutoModelForCausalLM.from_pretrained('gpt2-xl').to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoded_ids_align(inputs, decoded, outputs):\n",
    "    inputs_ids = inputs['input_ids'].squeeze()\n",
    "    # make outputs as keys of tok_ids\n",
    "    tok_ids = {}\n",
    "\n",
    "    for tok, ids in zip(decoded, inputs_ids):\n",
    "        if any(f in tok for f in outputs):\n",
    "            tok_ids[tok] = ids\n",
    "\n",
    "    return tok_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle a list n times\n",
    "def list_shuffle(lst, n):\n",
    "    import random\n",
    "    new_lsts = []\n",
    "    for _ in range(n):\n",
    "        lst_cp = lst.copy()\n",
    "        random.shuffle(lst_cp)\n",
    "        new_lsts.append(lst_cp)\n",
    "    return new_lsts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def load_data_json(file):\n",
    "    \"\"\"Load data from json file\"\"\"\n",
    "    with open(file, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "        prompt_data = data[\"prompts_list\"]\n",
    "    \n",
    "    prompt_ult = []\n",
    "    preds_ult = []\n",
    "    outputs_ult = []\n",
    "\n",
    "    for prompts in prompt_data:\n",
    "        prompt_shuffled = list_shuffle(prompts, 50)\n",
    "        prompt_list = []\n",
    "        preds = []\n",
    "        outputs = []\n",
    "        for prompt in prompt_shuffled:\n",
    "            outputs.append(set([de.split()[-1] for de in prompt]))\n",
    "            p = \" \".join(prompt[:-1])\n",
    "            last_prompt = prompt[-1].split()\n",
    "            for i in range(0, len(last_prompt)-1):\n",
    "                p += \" \" + last_prompt[i]\n",
    "            prompt_list.append(p)\n",
    "            preds.append(last_prompt[-1])\n",
    "        prompt_ult.append(prompt_list)\n",
    "        preds_ult.append(preds)\n",
    "        outputs_ult.append(outputs)\n",
    "    return prompt_ult, preds_ult, outputs_ult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def acc(datapath):\n",
    "    print(datapath)\n",
    "    prompt_list, labels_list, outputs_list = load_data_json(data_path)\n",
    "\n",
    "    def flatten(lst):\n",
    "        return [item for sublist in lst for item in sublist]\n",
    "    \n",
    "    prompt_list, labels_list, outputs_list = flatten(prompt_list), flatten(labels_list), flatten(outputs_list)\n",
    "    \n",
    "    preds = []\n",
    "    for prompt, label, output in tqdm(zip(prompt_list, labels_list, outputs_list)):\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "        decoded_tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "        tok_ids_dict = decoded_ids_align(inputs, decoded_tokens, output)\n",
    "        logits = model(**inputs).logits\n",
    "        scores = [logits[0, -1, list(tok_ids_dict.values())[i]].item() for i in range(len(tok_ids_dict))]\n",
    "        # get the index of the max score\n",
    "        max_score_index = scores.index(max(scores))\n",
    "        # get the token of the max score\n",
    "        max_score_token = list(tok_ids_dict.keys())[max_score_index]\n",
    "        # get the predicted token\n",
    "        max_score_token = next((o for o in output if o in max_score_token), max_score_token)\n",
    "        preds.append(max_score_token)\n",
    "\n",
    "    from sklearn.metrics import accuracy_score, f1_score\n",
    "    print(\"Acc: \", accuracy_score(labels_list, preds))\n",
    "    print(\"F1-macro: \", f1_score(labels_list, preds, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/s6-HH/selection_40.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "550it [00:17, 31.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc:  0.9109090909090909\n",
      "F1-macro:  0.9249437604875013\n",
      "data/s6-HH/selection_46.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "550it [00:18, 28.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc:  0.9327272727272727\n",
      "F1-macro:  0.9526607669220948\n",
      "data/s6-HH/selection_47.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "550it [00:19, 28.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc:  0.9127272727272727\n",
      "F1-macro:  0.9483595517244098\n",
      "data/s6-HH/selection_49.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "550it [00:17, 31.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc:  0.9290909090909091\n",
      "F1-macro:  0.9634400862159822\n",
      "data/s6-HH/selection_52.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "550it [00:17, 31.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc:  0.9272727272727272\n",
      "F1-macro:  0.9519855967473418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data_paths = [\n",
    "    'data/s6-HH/selection_40.json',\n",
    "    'data/s6-HH/selection_46.json',\n",
    "    'data/s6-HH/selection_47.json',\n",
    "    'data/s6-HH/selection_49.json',\n",
    "    'data/s6-HH/selection_52.json',\n",
    "\n",
    "]\n",
    "\n",
    "for data_path in data_paths:\n",
    "    acc(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/s5-HM/selection_6.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "550it [00:23, 23.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc:  0.9636363636363636\n",
      "F1-macro:  0.9712947731359167\n",
      "data/s5-HM/selection_8.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "550it [00:23, 23.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc:  0.9545454545454546\n",
      "F1-macro:  0.9557621285413874\n",
      "data/s5-HM/selection_9.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "550it [00:20, 26.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc:  0.9381818181818182\n",
      "F1-macro:  0.9178505781485234\n",
      "data/s5-HM/selection_11.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "550it [00:22, 24.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc:  0.96\n",
      "F1-macro:  0.9762243628971466\n",
      "data/s5-HM/selection_12.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "550it [00:23, 23.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc:  0.96\n",
      "F1-macro:  0.9688747011689649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data_paths = [\n",
    "    'data/s5-HM/selection_6.json',\n",
    "    'data/s5-HM/selection_8.json',\n",
    "    'data/s5-HM/selection_9.json',\n",
    "    'data/s5-HM/selection_11.json',\n",
    "    'data/s5-HM/selection_12.json',\n",
    "\n",
    "\n",
    "]\n",
    "\n",
    "for data_path in data_paths:\n",
    "    acc(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/s3-ML/selection_2.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "550it [00:28, 19.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19818181818181818\n",
      "data/s3-ML/selection_6.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "550it [00:27, 19.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.36\n",
      "data/s3-ML/selection_9.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "550it [00:28, 19.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.28\n",
      "data/s3-ML/selection_22.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "550it [00:28, 19.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24363636363636362\n",
      "data/s3-ML/selection_23.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "550it [00:27, 19.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2636363636363636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data_paths = [\n",
    "    'data/s3-ML/selection_2.json',\n",
    "    'data/s3-ML/selection_6.json',\n",
    "    'data/s3-ML/selection_9.json',\n",
    "    'data/s3-ML/selection_22.json',\n",
    "    'data/s3-ML/selection_23.json',\n",
    "]\n",
    "\n",
    "for data_path in data_paths:\n",
    "    acc(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/s2-M/selection_6.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "550it [00:12, 45.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6181818181818182\n",
      "data/s2-M/selection_9.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "550it [00:12, 45.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6381818181818182\n",
      "data/s2-M/selection_17.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "550it [00:11, 45.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6254545454545455\n",
      "data/s2-M/selection_21.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "550it [00:12, 45.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6036363636363636\n",
      "data/s2-M/selection_22.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "550it [00:12, 45.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5781818181818181\n",
      "data/s2-M/selection_40.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "550it [00:12, 45.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5927272727272728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data_paths = [\n",
    "    'data/s2-M/selection_6.json',\n",
    "    'data/s2-M/selection_9.json',\n",
    "    'data/s2-M/selection_17.json',\n",
    "    'data/s2-M/selection_21.json',\n",
    "    'data/s2-M/selection_22.json',\n",
    "    'data/s2-M/selection_40.json',\n",
    "]\n",
    "\n",
    "for data_path in data_paths:\n",
    "    acc(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/s1-L/selection_7.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "550it [00:32, 16.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11272727272727273\n",
      "data/s1-L/selection_12.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "550it [00:30, 18.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.28545454545454546\n",
      "data/s1-L/selection_14.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "550it [00:30, 17.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14363636363636365\n",
      "data/s1-L/selection_35.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "550it [00:28, 19.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11818181818181818\n",
      "data/s1-L/selection_40.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "550it [00:28, 19.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.26545454545454544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data_paths = [\n",
    "    'data/s1-L/selection_7.json',\n",
    "    'data/s1-L/selection_12.json',\n",
    "    'data/s1-L/selection_14.json',\n",
    "    'data/s1-L/selection_35.json',\n",
    "    'data/s1-L/selection_40.json',\n",
    "]\n",
    "\n",
    "for data_path in data_paths:\n",
    "    acc(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/s4-HL/selection_8.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "550it [00:23, 23.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5709090909090909\n",
      "data/s4-HL/selection_27.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "550it [00:27, 19.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5145454545454545\n",
      "data/s4-HL/selection_36.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "550it [00:24, 22.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5018181818181818\n",
      "data/s4-HL/selection_43.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "550it [00:28, 19.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5418181818181819\n",
      "data/s4-HL/selection_52.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "550it [00:24, 22.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7545454545454545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data_paths = [\n",
    "    'data/s4-HL/selection_8.json',\n",
    "    'data/s4-HL/selection_27.json',\n",
    "    'data/s4-HL/selection_36.json',\n",
    "    'data/s4-HL/selection_43.json',\n",
    "    'data/s4-HL/selection_52.json',\n",
    "]\n",
    "\n",
    "for data_path in data_paths:\n",
    "    acc(data_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icl_mi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
